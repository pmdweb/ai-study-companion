# ğŸ¯ AI Study Companion - Interview Mentor

> Transform weak interview responses into compelling STAR-format answers with AI-powered feedback.
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/framework-Streamlit-red.svg)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
## âœ¨ Features

- **ğŸ“Š STAR Evaluation**: Comprehensive scoring of Situation, Task, Action, Result components
- **ğŸš€ AI Enhancement**: Automatically rewrite weak responses into professional STAR format
- **ğŸ¤” Follow-up Questions**: Generate realistic recruiter follow-up questions for practice
- **ğŸ“„ Session Export**: Save complete interview sessions with original and enhanced responses
- **ğŸ”„ Score Comparison**: See improvement metrics before and after enhancement
- **ğŸ  Local-First**: No external API dependencies, runs completely offline

## ğŸš€ Quick Start
### Prerequisites
Open your browser to `http://localhost:8501`.
# ğŸ¯ AI Study Companion - Interview Mentor

> Transform interview responses into polished STAR-format answers with actionable AI feedback.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/framework-Streamlit-red.svg)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- **ğŸ“Š STAR Evaluation**: Scores Situation, Task, Action, Result components (0-2 each).
- **ğŸš€ AI Enhancement**: Rewrites answers into concise, professional STAR responses.
- **ğŸ¤” Follow-up Questions**: Generates realistic recruiter follow-ups for practice.
- **ğŸ“„ Session Export**: Save interview sessions (original + enhanced answers).
- **ğŸ”„ Score Comparison**: See before/after improvement metrics.
- **ğŸ  Local-First**: Runs offline with local models and no external API keys.

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Git

### Installation (Windows PowerShell)

```powershell
# Clone the repository
- Git

# Create and activate a virtual environment
python -m venv .venv; .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
python -m spacy download en_core_web_sm

# Run the app
streamlit run app.py
```

Open your browser to http://localhost:8501

## ğŸ¯ How It Works

1. Select a sample question or add your own.
2. Write your response in the text area.
3. Click "Evaluate Response" to receive STAR scores and targeted feedback.
4. Click "Generate Improved Response" to get a polished STAR answer.
5. Export the session for review.

## ğŸ—ï¸ Architecture

### Tech Stack

- UI: Streamlit
- NLP: spaCy + sentence-transformers
- Embeddings: all-MiniLM-L6-v2
- Vector DB: ChromaDB (local)
- PDF Processing: PyMuPDF (planned)
- LLMs: Template-based prompts with optional local models

### Project Structure

```text
ai-study-companion/
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ requirements.txt       # Python deps
â”œâ”€â”€ .github/               # Agent instructions & CI
â”œâ”€â”€ data/                  # PDFs and vector DB
â””â”€â”€ src/                   # Core modules (star_rubric, rag, embedder)
```

## ğŸ¯ STAR Methodology

The evaluator checks:

- Situation: context and setup
- Task: objective and responsibility
- Action: concrete steps and verbs
- Result: quantifiable outcomes or impact

Each item scores 0-2 (max 8). Missing metrics in Result reduce the score.

## ğŸ”§ Development

### Multi-Agent Development

The repo includes guidance for multiple specialized agents (frontend, backend, docs, presentation). See `.github/` for instructions.

### Running Tests

```bash
# Quick smoke tests
python -c "from src.star_rubric import score_star; print(score_star('I fixed a bug by improving logging'))"
```

### Dev Commands

```bash
# PDF ingest (planned)
python src/ingest_pdf.py path/to/questions.pdf
python src/embedder.py

# Start dev server
streamlit run app.py --server.runOnSave true
```

## ğŸ“ˆ Roadmap

- PDF question bank upload & indexing
- Audio recording & transcription
- Local LLM integrations (flan-t5, mistral)
- Industry-specific templates
- Progress tracking dashboard

## ğŸ¤ Contributing

1. Fork the repo
2. Create a branch: git checkout -b feature/x
3. Commit & push
4. Open a PR

Code style: follow PEP 8 and add docstrings to core modules.

## ğŸ“ License

MIT. See LICENSE file.
# Run the application
# ğŸ¯ AI Study Companion - Interview Mentor

> Transform interview responses into polished STAR-format answers with actionable AI feedback.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/framework-Streamlit-red.svg)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- **ğŸ“Š STAR Evaluation** â€” Scores Situation, Task, Action, Result components (0-2 each).
- **ğŸš€ AI Enhancement** â€” Rewrites answers into concise, professional STAR responses.
- **ğŸ¤” Follow-up Questions** â€” Generates realistic recruiter follow-ups for practice.
- **ğŸ“„ Session Export** â€” Save interview sessions (original + enhanced answers).
- **ğŸ”„ Score Comparison** â€” See before/after improvement metrics.
- **ğŸ  Local-First** â€” Runs offline with local models and no external API keys.

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Git

### Installation (Windows PowerShell)

```powershell
# Clone the repository
git clone https://github.com/yourusername/ai-study-companion.git; cd ai-study-companion

# Create and activate a virtual environment
python -m venv .venv; .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
python -m spacy download en_core_web_sm

# Run the app
streamlit run app.py
```

Open your browser to [http://localhost:8501](http://localhost:8501).

## ğŸ¯ How It Works

1. Select a sample question or add your own.
2. Write your response in the text area.
3. Click "Evaluate Response" to receive STAR scores and targeted feedback.
4. Click "Generate Improved Response" to get a polished STAR answer.
5. Export the session for review.

## ğŸ—ï¸ Architecture

### Tech Stack

- UI: Streamlit
- NLP: spaCy + sentence-transformers
- Embeddings: all-MiniLM-L6-v2
- Vector DB: ChromaDB (local)
- PDF Processing: PyMuPDF (planned)
- LLMs: Template-based prompts with optional local models

### Project Structure

```text
ai-study-companion/
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ requirements.txt       # Python deps
â”œâ”€â”€ .github/               # Agent instructions & CI
â”œâ”€â”€ data/                  # PDFs and vector DB
â””â”€â”€ src/                   # Core modules (star_rubric, rag, embedder)
```

## ğŸ¯ STAR Methodology

The evaluator checks:

- Situation: context and setup
- Task: objective and responsibility
- Action: concrete steps and verbs
- Result: quantifiable outcomes or impact

Each item scores 0-2 (max 8). Missing metrics in Result reduce the score.

## ğŸ”§ Development

### Multi-Agent Development

The repo includes guidance for multiple specialized agents (frontend, backend, docs, presentation). See `.github/` for instructions.

### Running Tests

```bash
# Quick smoke tests
python -c "from src.star_rubric import score_star; print(score_star('I fixed a bug by improving logging'))"
```

### Dev Commands

```bash
# PDF ingest (planned)
python src/ingest_pdf.py path/to/questions.pdf
python src/embedder.py

# Start dev server
streamlit run app.py --server.runOnSave true
```

## ğŸ“ˆ Roadmap

- PDF question bank upload & indexing
- Audio recording & transcription
- Local LLM integrations (flan-t5, mistral)
- Industry-specific templates
- Progress tracking dashboard

## ğŸ¤ Contributing

1. Fork the repo
2. Create a branch: `git checkout -b feature/x`
3. Commit & push
4. Open a PR

Code style: follow PEP 8 and add docstrings to core modules.

## ğŸ“ License

MIT. See LICENSE file.

---

Focus: STAR methodology accuracy and realistic recruiter-style feedback. This tool is for interview preparation, not general study use.

- Follow PEP 8 for Python code
- Use meaningful variable names
- Add docstrings to functions
- Test STAR evaluation accuracy

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [Streamlit](https://streamlit.io/) for rapid prototyping
- Uses [sentence-transformers](https://www.sbert.net/) for semantic search
- Inspired by the STAR methodology for behavioral interviews
- Multi-agent development pattern for efficient collaboration

## ğŸ“ Support

If you have questions or need help:

1. Check the [Issues](https://github.com/yourusername/ai-study-companion/issues) page
2. Review the `.github/` directory for agent-specific guidance
3. Open a new issue with detailed information

---

**Focus on STAR methodology accuracy and interview realism. This is interview preparation, not a general study tool.** ğŸ¯